{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model averaging can be improved by weighting the contributions of each sub-model to the combined prediction by the expected performance of the submodel. This can be extended further by training an entirely new model to learn how to best combine the contributions from each submodel. This approach is called stacked generalization, or stacking for short, and can result in better predictive performance than any single contributing model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stacked generalization is an ensemble method where a new model learns how to best combine the predictions from multiple  existing models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://machinelearningmastery.com/stacking-ensemble-for-deep-learning-neural-networks/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import MaxPool2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.layers import Softmax\n",
    "from keras.layers import Dropout\n",
    "from keras import regularizers\n",
    "from keras import Sequential\n",
    "from keras.models import load_model\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.preprocessing import scale\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.callbacks import Callback, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import roc_auc_score, auc\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "read = lambda imname: np.array(Image.open(imname).convert('RGB'))\n",
    "benign_label = 1.0\n",
    "malignant_label = 0.0\n",
    "RESIZE = 128\n",
    "test_size = 0.4\n",
    "learning_rate = 3e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 121.80it/s]\n"
     ]
    }
   ],
   "source": [
    "benign_files = []\n",
    "benign_labels = []\n",
    "path = '/Applications/GIAP/hp/BreaKHis_v1/histology_slides/breast/benign/SOB'\n",
    "for TYPE_NAME in tqdm.tqdm(os.listdir(path)):\n",
    "    if TYPE_NAME == '.DS_Store':\n",
    "        continue\n",
    "    FILE_PATH = os.path.join(path, TYPE_NAME)\n",
    "    for SUBFILE_NAME in os.listdir(FILE_PATH):\n",
    "        if SUBFILE_NAME == '.DS_Store':\n",
    "            continue\n",
    "        SUBFILE_PATH = os.path.join(FILE_PATH, SUBFILE_NAME)\n",
    "        for ZOOM_FILE in os.listdir(SUBFILE_PATH):\n",
    "            if ZOOM_FILE == '.DS_Store':\n",
    "                continue\n",
    "            ZOOM_PATH = os.path.join(SUBFILE_PATH, ZOOM_FILE)\n",
    "            for IMAGE_NAME in os.listdir(ZOOM_PATH):\n",
    "                IMAGE_PATH = os.path.join(ZOOM_PATH, IMAGE_NAME)\n",
    "                benign_files.append(IMAGE_PATH)\n",
    "                benign_labels.append(benign_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 70.56it/s]\n"
     ]
    }
   ],
   "source": [
    "malignant_files = []\n",
    "malignant_labels = []\n",
    "path = '/Applications/GIAP/hp/BreaKHis_v1/histology_slides/breast/malignant/SOB'\n",
    "for TYPE_NAME in tqdm.tqdm(os.listdir(path)):\n",
    "    FILE_PATH = os.path.join(path, TYPE_NAME)\n",
    "    for SUBFILE_NAME in os.listdir(FILE_PATH):\n",
    "        SUBFILE_PATH = os.path.join(FILE_PATH, SUBFILE_NAME)\n",
    "        for ZOOM_FILE in os.listdir(SUBFILE_PATH):\n",
    "            ZOOM_PATH = os.path.join(SUBFILE_PATH, ZOOM_FILE)\n",
    "            for IMAGE_NAME in os.listdir(ZOOM_PATH):\n",
    "                IMAGE_PATH = os.path.join(ZOOM_PATH, IMAGE_NAME)\n",
    "                malignant_files.append(IMAGE_PATH)\n",
    "                malignant_labels.append(malignant_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = benign_files + malignant_files\n",
    "labels = benign_labels + malignant_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "\n",
    "for file in files:\n",
    "    img = read(file)\n",
    "    img = cv2.resize(img, (RESIZE, RESIZE))\n",
    "    img = img / 255.0\n",
    "    images.append(np.array(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "npimages = np.array(images)\n",
    "nplabels = np.array(labels)\n",
    "\n",
    "shuffle = np.arange(npimages.shape[0])\n",
    "np.random.shuffle(shuffle)\n",
    "x = npimages[shuffle]\n",
    "y = nplabels[shuffle]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain, xtestPLUShold, ytrain, ytestPLUShold = train_test_split(x, y, test_size=test_size, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtest, xhold, ytest, yhold = train_test_split(xtestPLUShold, ytestPLUShold, test_size=0.5, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_models(model_files):\n",
    "    all_model = []\n",
    "    for file in model_files:\n",
    "        model = load_model(file)\n",
    "        all_model.append(model)\n",
    "\n",
    "    return all_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-20 17:56:29.911355: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "model_files = ['custom_binary_model.h5', 'vgg_model.h5']\n",
    "members = load_all_models(model_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 0.855\n",
      "Model Accuracy: 0.942\n"
     ]
    }
   ],
   "source": [
    "for model in members:\n",
    "    _, accuracy = model.evaluate(xtest, ytest, verbose=0)\n",
    "    print('Model Accuracy: %.3f' % accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stacked_dataset(members, xinput):\n",
    "    xstack = None\n",
    "    for model in members:\n",
    "        yhat = model.predict(xinput, verbose=0)\n",
    "        if xstack is None:\n",
    "            xstack = yhat\n",
    "        else:\n",
    "            xstack = np.dstack((xstack, yhat))\n",
    "\n",
    "    xstack = xstack.reshape((xstack.shape[0], xstack.shape[1]*xstack.shape[2]))\n",
    "    return xstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_stacked_model(members, xtest, ytest):\n",
    "    xstacked = stacked_dataset(members, xtest)\n",
    "    model = LogisticRegression()\n",
    "    model.fit(xstacked, ytest)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fit_stacked_model(members, xtest, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stacked_prediction(members, model, xhold):\n",
    "    xstacked = stacked_dataset(members, xhold)\n",
    "    yhat = model.predict(xstacked)\n",
    "    return yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacked Test Accuracy: 0.952\n"
     ]
    }
   ],
   "source": [
    "yhat = stacked_prediction(members, model, xhold)\n",
    "accuracy = accuracy_score(yhold, yhat)\n",
    "print('Stacked Test Accuracy: %.3f' % accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
